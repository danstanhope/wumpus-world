{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fl0gJZgb0aa0"
   },
   "outputs": [],
   "source": [
    "class North:\n",
    "    def turnLeft(self):\n",
    "        return West\n",
    "\n",
    "    def turnRight(self):\n",
    "        return East\n",
    "\n",
    "class South:\n",
    "    def turnLeft(self):\n",
    "        return East\n",
    "\n",
    "    def turnRight(self):\n",
    "        return West\n",
    "\n",
    "class East:\n",
    "    def turnLeft(self):\n",
    "        return North\n",
    "\n",
    "    def turnRight(self):\n",
    "        return South\n",
    "\n",
    "class West:\n",
    "    def turnLeft(self):\n",
    "        return South\n",
    "\n",
    "    def turnRight(self):\n",
    "        return North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u-g25Lkm0BDs"
   },
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "class Action(Enum):\n",
    "    Forward = \"Forward\"\n",
    "    TurnLeft = \"TurnLeft\"\n",
    "    TurnRight = \"TurnRight\"\n",
    "    Shoot = \"Shoot\"\n",
    "    Grab = \"Grab\"\n",
    "    Climb = \"Climb\"\n",
    "\n",
    "\n",
    "class Coords:\n",
    "    def __init__(self, x, y):\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def adjacentCells(self, gridWidth, gridHeight):\n",
    "        toLeft = Coords(self._x - 1, self._y ) if (self._x > 0) else False\n",
    "        toRight =  Coords(self._x + 1, self._y ) if (self._x < gridWidth - 1) else False\n",
    "        below = Coords(self._x, self._y  - 1) if (self._y  > 0) else False\n",
    "        above = Coords(self._x, self._y  + 1) if (self._y  < gridHeight - 1) else False\n",
    "\n",
    "        return [toLeft, toRight, below, above]\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self._x\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "\n",
    "class Percept:\n",
    "    def __init__(self, stench, breeze, glitter, bump, scream, isTerminated, reward):\n",
    "        self.stench = stench\n",
    "        self.breeze = breeze\n",
    "        self.glitter = glitter\n",
    "        self.bump = bump\n",
    "        self.scream = scream\n",
    "        self.isTerminated = isTerminated\n",
    "        self.reward = reward\n",
    "\n",
    "    def show(self):\n",
    "        return \"stench:%s breeze:%s glitter:%s bump:%s scream:%s isTerminated:%s reward:%s\" % (self.stench, self.breeze, self.glitter, self.bump, self.scream, self.isTerminated, self.reward)\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self,\n",
    "                 gridWidth,\n",
    "                 gridHeight,\n",
    "                 pitProb,\n",
    "                 allowClimbWithoutGold,\n",
    "                 agent,\n",
    "                 pitLocations,\n",
    "                 isTerminated,\n",
    "                 wumpusLocation,\n",
    "                 wumpusAlive,\n",
    "                 goldLocation):\n",
    "        self.gridWidth = gridWidth\n",
    "        self.gridHeight = gridHeight\n",
    "        self.pitProb = pitProb\n",
    "        self.allowClimbWithoutGold = allowClimbWithoutGold\n",
    "        self.agent = agent\n",
    "        self.pitLocations = pitLocations\n",
    "        self.isTerminated = isTerminated\n",
    "        self.wumpusLocation = wumpusLocation\n",
    "        self.wumpusAlive = wumpusAlive\n",
    "        self.goldLocation = goldLocation\n",
    "\n",
    "    def isPitAt(self, coords):\n",
    "        for item in self.pitLocations:\n",
    "            if(item.x == coords.x and item.y == coords.y):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def isWumpusAt(self, coords):\n",
    "        if(self.wumpusLocation is None):\n",
    "            return False\n",
    "        elif(self.wumpusLocation.x == coords.x and self.wumpusLocation.y == coords.y):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isAgentAt(self, coords):\n",
    "        if(self.agent.location.x == coords.x and self.agent.location.y == coords.y):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isGlitter(self):\n",
    "        if(self.goldLocation is None):\n",
    "            return False\n",
    "        elif(self.agent.location.x == self.goldLocation.x and self.agent.location.y == self.goldLocation.y):\n",
    "            return True\n",
    "        else:\n",
    "            return False           \n",
    "\n",
    "    def isGoldAt(self, coords):\n",
    "        if(self.goldLocation is None):\n",
    "            return False\n",
    "        elif(coords.x == self.goldLocation.x and coords.y == self.goldLocation.y):\n",
    "            return True\n",
    "        else:\n",
    "            return False         \n",
    "\n",
    "    def killAttemptSuccessful(self):\n",
    "        if(self.wumpusLocation is None):\n",
    "            return True\n",
    "\n",
    "        wumpusInLineOfFire = False\n",
    "        \n",
    "        if self.agent.orientation == West:\n",
    "            if (self.agent.location.x > self.wumpusLocation.x and self.agent.location.y == self.wumpusLocation.y):\n",
    "                wumpusInLineOfFire = True\n",
    "        elif self.agent.orientation == East:\n",
    "            if (self.agent.location.x < self.wumpusLocation.x and self.agent.location.y == self.wumpusLocation.y):\n",
    "                wumpusInLineOfFire = True\n",
    "        elif self.agent.orientation == South:\n",
    "            if (self.agent.location.x == self.wumpusLocation.x and self.agent.location.y > self.wumpusLocation.y):\n",
    "                wumpusInLineOfFire = True\n",
    "        elif self.agent.orientation == North:\n",
    "            if (self.agent.location.x == self.wumpusLocation.x and self.agent.location.y < self.wumpusLocation.y):\n",
    "                wumpusInLineOfFire = True\n",
    "        \n",
    "        return self.agent.hasArrow and self.wumpusAlive and wumpusInLineOfFire\n",
    "\n",
    "    def adjacentCells(self, coords):\n",
    "        toLeft = Coords(coords.x - 1, coords.y) if (coords.x > 0) else False\n",
    "        toRight =  Coords(coords.x + 1, coords.y) if (coords.x < self.gridWidth - 1) else False\n",
    "        below = Coords(coords.x, coords.y - 1) if (coords.y > 0) else False\n",
    "        above = Coords(coords.x, coords.y + 1) if (coords.y < self.gridHeight - 1) else False\n",
    "\n",
    "        return [toLeft, toRight, below, above]\n",
    "\n",
    "    def isPitAdjacent(self, coords):\n",
    "        adjacent = self.adjacentCells(coords)\n",
    "        for item in adjacent:\n",
    "            if(item != False):\n",
    "                if(self.isPitAt(item)):\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "    def isWumpusAdjacent(self, coords):\n",
    "        adjacent = self.adjacentCells(coords)\n",
    "\n",
    "        for item in adjacent:\n",
    "            if(item != False):\n",
    "                if(self.isWumpusAt(item)):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def isBreeze(self):\n",
    "        return self.isPitAdjacent(self.agent.location)\n",
    "\n",
    "    def isStench(self):\n",
    "        return self.isWumpusAdjacent(self.agent.location) or self.isWumpusAt(self.agent.location)\n",
    "\n",
    "    def applyAction(self, action):\n",
    "        if (self.isTerminated):\n",
    "            return self, Percept(False, False, False, False, False, True, 0)\n",
    "        else:\n",
    "            if action.name is Action.Forward.name:                \n",
    "                movedAgent = self.agent.forward(\n",
    "                    self.gridWidth, self.gridHeight)\n",
    "\n",
    "                death = (self.isWumpusAt(movedAgent.location)\n",
    "                         and self.wumpusAlive) or self.isPitAt(movedAgent.location)\n",
    "                \n",
    "                newAgent = deepcopy(movedAgent)\n",
    "                newAgent.isAlive = False if death else True\n",
    "\n",
    "                Bump = False\n",
    "                if(newAgent.location.x == self.agent.location.x and newAgent.location.y == self.agent.location.y):\n",
    "                    Bump = True\n",
    "                \n",
    "                newEnv = Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, newAgent, self.pitLocations,\n",
    "                                     death, self.wumpusLocation, self.wumpusAlive, newAgent.location if self.agent.hasGold else self.goldLocation)\n",
    "                return newEnv, Percept(newEnv.isStench(), newEnv.isBreeze(), newEnv.isGlitter(), Bump, False, False if newAgent.isAlive else True,  -1 if newAgent.isAlive else -1001)\n",
    "            elif action.name is Action.TurnLeft.name:\n",
    "                return Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, self.agent.turnLeft(), self.pitLocations, self.isTerminated,  self.wumpusLocation,  self.wumpusAlive,  self.goldLocation), Percept(self.isStench(), self.isBreeze(), self.isGlitter(), False, False,  False, -1)\n",
    "            elif action.name is Action.TurnRight.name:\n",
    "                return Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, self.agent.turnRight(), self.pitLocations, self.isTerminated, self.wumpusLocation, self.wumpusAlive, self.goldLocation), Percept(self.isStench(), self.isBreeze(), self.isGlitter(), False, False,  False, -1)\n",
    "            elif action.name is Action.Grab.name:\n",
    "                newAgent = deepcopy(self.agent)\n",
    "                newAgent.hasGold = self.isGlitter()\n",
    "\n",
    "                return Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, newAgent, self. pitLocations, self.isTerminated, self.wumpusLocation, self.wumpusAlive, self.agent.location if newAgent.hasGold else self.goldLocation), Percept(self.isStench(), self.isBreeze(), self.isGlitter(), False, False,  False, -1)\n",
    "            elif action.name is Action.Climb.name:\n",
    "                inStartLocation = False\n",
    "                if(self.agent.location.x == Coords(0, 0).x and self.agent.location.y == Coords(0, 0).y):\n",
    "                    inStartLocation = True\n",
    "\n",
    "                success = self.agent.hasGold and inStartLocation\n",
    "                isTerminated = success or self.allowClimbWithoutGold\n",
    "\n",
    "                return Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, self.agent, self.pitLocations, isTerminated, self.wumpusLocation, self.wumpusAlive, self.goldLocation), Percept(False, False, self.agent.hasGold, False, False, isTerminated, 999 if success else -1)\n",
    "            elif action.name is Action.Shoot.name:\n",
    "                hadArrow = self.agent.hasArrow\n",
    "                wumpusKilled = self.killAttemptSuccessful()\n",
    "                newAgent = deepcopy(self.agent)\n",
    "                newAgent.hasArrow = False\n",
    "                return Environment(self.gridWidth, self.gridHeight, self.pitProb, self.allowClimbWithoutGold, newAgent, self.pitLocations, self.isTerminated, self.wumpusLocation, self.wumpusAlive and not wumpusKilled, self.goldLocation), Percept(self.isStench(), self.isBreeze(), self.isGlitter(), False, wumpusKilled, False, -11 if hadArrow else -1)\n",
    "\n",
    "    def visualize(self):\n",
    "        wumpusSymbol =  \"W\" if (self.wumpusAlive == True) else \"w\"         \n",
    "\n",
    "        Rows = []\n",
    "        for y in range(self.gridHeight):\n",
    "            Cells = []\n",
    "            for x in range(self.gridWidth):\n",
    "                A = \"A\" if (self.isAgentAt(Coords(x, y))) else \" \"\n",
    "                P = \"P\" if (self.isPitAt(Coords(x, y))) else \" \"\n",
    "                G = \"G\" if (self.isGoldAt(Coords(x, y))) else \" \"\n",
    "                W = wumpusSymbol if (self.isWumpusAt(Coords(x, y))) else \" \"\n",
    "                \n",
    "                Cells.append(\"%s%s%s%s\" % (A, P, G, W))\n",
    "\n",
    "            Rows.append('|'.join(Cells))\n",
    "\n",
    "        return '\\n'.join(Rows)\n",
    "\n",
    "class WumpusWorldEnvironment:\n",
    "    def apply(self, gridWidth, gridHeight, pitProb, allowClimbWithoutGold):\n",
    "        self.gridWidth = gridWidth\n",
    "        self.gridHeight = gridHeight\n",
    "        self.pitProb = pitProb\n",
    "        self.allowClimbWithoutGold = allowClimbWithoutGold\n",
    "\n",
    "        cellIndexes = []\n",
    "        pitLocations = []\n",
    "\n",
    "        for x in range(gridWidth):\n",
    "            for y in range(gridHeight):\n",
    "                cellIndexes.append(Coords(x, y))\n",
    "\n",
    "        cellIndexes.pop(0)\n",
    "        pitCount = 0\n",
    "        for item in cellIndexes:\n",
    "            if random.uniform(0, 1) < pitProb:\n",
    "                if(pitCount == 3):\n",
    "                    break\n",
    "                pitLocations.append(item)\n",
    "                pitCount = pitCount + 1\n",
    "\n",
    "        env = Environment(\n",
    "            gridWidth,\n",
    "            gridHeight,\n",
    "            pitProb,\n",
    "            allowClimbWithoutGold,\n",
    "            Agent(),\n",
    "            pitLocations,\n",
    "            False,\n",
    "            self.randomLocationExceptOrigin(),\n",
    "            True,\n",
    "            self.randomLocationExceptOrigin()\n",
    "        )\n",
    "\n",
    "        return env, Percept(env.isStench(), env.isBreeze(), False, False, False, False,  0.0)\n",
    "\n",
    "    def randomLocationExceptOrigin(self):\n",
    "        x = random.randrange(self.gridWidth)\n",
    "        y = random.randrange(self.gridHeight)\n",
    "\n",
    "        if x == 0 and y == 0:\n",
    "            self.randomLocationExceptOrigin()\n",
    "        else:\n",
    "            return Coords(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AqrL7w4E0fYA"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, location = Coords(0,0), orientation = East, hasGold = False, hasArrow = True, isAlive = True):\n",
    "        self.location =  location\n",
    "        self.orientation = orientation\n",
    "        self.hasGold = hasGold\n",
    "        self.hasArrow = hasArrow\n",
    "        self.isAlive = isAlive\n",
    "\n",
    "    def turnLeft(self):\n",
    "        ret = deepcopy(self)\n",
    "        ret.orientation = self.orientation.turnLeft(self)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def turnRight(self):\n",
    "        ret = deepcopy(self)\n",
    "        ret.orientation  = self.orientation.turnRight(self)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def useArrow(self):\n",
    "        ret = deepcopy(self)\n",
    "        ret.hasArrow = False\n",
    "    \n",
    "        return ret\n",
    "\n",
    "    def forward(self, gridWidth, gridHeight):\n",
    "        ret = deepcopy(self)\n",
    "        \n",
    "        newAgentLocation = False\n",
    "\n",
    "        if self.orientation == West:\n",
    "            newAgentLocation = Coords(max(0, self.location.x - 1), self.location.y)\n",
    "        elif self.orientation == East:\n",
    "            newAgentLocation = Coords(min(gridWidth - 1, self.location.x + 1), self.location.y)\n",
    "        elif self.orientation == South:\n",
    "            newAgentLocation = Coords(self.location.x, max(0, self.location.y - 1))\n",
    "        elif self.orientation == North:\n",
    "            newAgentLocation = Coords(self.location.x, min(gridHeight - 1, self.location.y + 1))\n",
    "\n",
    "\n",
    "        ret.location = newAgentLocation\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def applyMoveAction(self, action, gridWidth, gridHeight):\n",
    "        if action == Action.Forward:\n",
    "            return self.forward(gridWidth, gridHeight)\n",
    "        elif action == Action.TurnRight:\n",
    "            return self.turnRight()\n",
    "        elif action == Action.TurnLeft:\n",
    "            return self.turnLeft()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4DxIXwnq0tIT"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from random import randrange\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\".\")\n",
    "\n",
    "\n",
    "class DeepQAgent():\n",
    "    def __init__(self, gridHeight=4, gridWidth=4, agentState=Agent, safeLocations=[Coords(0,0)], stenchLocations=[], breezeLocations=[], agentLocationGrid = [], safeLocationGrid = [], stenchLocationGrid = [], breezeLocationGrid = [], agentHasGold = False, agentSensesGold = False, agentHasArrow = False, agentHeardScream = False, agentOrientationSet= False):\n",
    "        self.gridHeight = gridHeight\n",
    "        self.gridWidth = gridWidth\n",
    "        self.agentState = agentState()\n",
    "        self.safeLocations = safeLocations\n",
    "        self.stenchLocations = stenchLocations\n",
    "        self.breezeLocations = breezeLocations\n",
    "        self.agentLocationGrid = agentLocationGrid\n",
    "        self.safeLocationGrid = safeLocationGrid\n",
    "        self.stenchLocationGrid = stenchLocationGrid\n",
    "        self.breezeLocationGrid = breezeLocationGrid\n",
    "        self.agentHasGold = agentHasGold\n",
    "        self.agentSensesGold = agentSensesGold\n",
    "        self.agentHasArrow = agentHasArrow\n",
    "        self.agentHeardScream = agentHeardScream\n",
    "        self.agentOrientationSet = agentOrientationSet\n",
    "\n",
    "    #Helper method to print grids nicely\n",
    "    def printTable(self, grid):\n",
    "        rows = []\n",
    "        for i in range(self.gridHeight):\n",
    "            cells = []\n",
    "            for j in range(self.gridWidth):            \n",
    "                cells.append(\"%s\" % (grid[i][j]))\n",
    "\n",
    "            rows.append('|'.join(cells))\n",
    "        return '\\n'.join(rows)\n",
    "\n",
    "    #Helper method to print all the different belief state values\n",
    "    def printBeliefState(self, percept):\n",
    "        print(\"-- Percept --\")\n",
    "        print(percept.show())\n",
    "\n",
    "        print('-- Current Agent Location --')\n",
    "        print(self.printTable(self.agentLocationGrid))\n",
    "\n",
    "        print('-- Agent Orientation Set --')\n",
    "        print(self.agentOrientationSet)        \n",
    "\n",
    "        print('-- Safe Locations --')\n",
    "        print(self.printTable(self.safeLocationGrid))\n",
    "\n",
    "        print('-- Stench Locations --')\n",
    "        print(self.printTable(self.stenchLocationGrid))\n",
    "\n",
    "        print('-- Breeze Locations --')\n",
    "        print(self.printTable(self.breezeLocationGrid))    \n",
    "\n",
    "        print('-- Agent Has Gold --')\n",
    "        print(self.agentHasGold)  \n",
    "\n",
    "        print('-- Agent Senses Gold --')\n",
    "        print(self.agentSensesGold)  \n",
    "\n",
    "        print('-- Agent Has Arrow --')\n",
    "        print(self.agentHasArrow)  \n",
    "\n",
    "        print('-- Agent Heard Scream --')\n",
    "        print(self.agentHeardScream)          \n",
    "\n",
    "    #flattening and concatenating all the arrays to one array of length 72\n",
    "    def getAgentBeliefState(self):               \n",
    "        return np.concatenate((np.array(self.agentLocationGrid).flatten(), np.array(self.safeLocationGrid).flatten(),np.array(self.stenchLocationGrid).flatten(), np.array(self.breezeLocationGrid).flatten(), np.array(self.agentOrientationSet).flatten(), np.array(self.agentHasGold).flatten(), np.array(self.agentSensesGold).flatten(), np.array(self.agentHasArrow).flatten(), np.array(self.agentHeardScream).flatten()))\n",
    "\n",
    "    #Set is represented as a list of 1s and 0s\n",
    "    #I don't think the order matters, but we'll say:\n",
    "    #  [1, 0, 0, 0] = North\n",
    "    #  [0, 1, 0, 0] = South\n",
    "    #  [0, 0, 1, 0] = East\n",
    "    #  [0, 0, 0, 1] = West\n",
    "    def buildAgentOrientatioSet(self, orientation):\n",
    "        orientationSet = []\n",
    "\n",
    "        if orientation == North:\n",
    "            orientationSet.append([1, 0, 0, 0])\n",
    "        elif orientation == South:\n",
    "            orientationSet.append([0, 1, 0, 0])\n",
    "        elif orientation == East:\n",
    "            orientationSet.append([0, 0, 1, 0])\n",
    "        elif orientation == West:\n",
    "            orientationSet.append([0, 0, 0, 1])\n",
    "\n",
    "        return orientationSet\n",
    "\n",
    "    #Build agent location grid\n",
    "    def buildAgentLocationGrid(self, coords):\n",
    "        rows = []\n",
    "\n",
    "        for i in range(self.gridHeight):\n",
    "            cols = []\n",
    "            for j in range(self.gridWidth):\n",
    "                if(i == coords.y and j == coords.x):\n",
    "                    cols.append(1)\n",
    "                else:\n",
    "                    cols.append(0)   \n",
    "\n",
    "            rows.append(cols)\n",
    "\n",
    "        return rows\n",
    "    \n",
    "    #Build safe location grid\n",
    "    def buildSafeLocationGrid(self, visited):\n",
    "        rows = []\n",
    "\n",
    "        for i in range(self.gridHeight):\n",
    "            cols = []\n",
    "            for j in range(self.gridWidth):\n",
    "                if any(d.y == i and d.x == j for d in visited):\n",
    "                    cols.append(1)\n",
    "                else:\n",
    "                    cols.append(0)   \n",
    "\n",
    "            rows.append(cols)\n",
    "\n",
    "        return rows            \n",
    "\n",
    "    #Build stench location grid\n",
    "    def buildStenchLocationGrid(self, stenches):\n",
    "        rows = []\n",
    "\n",
    "        for i in range(self.gridHeight):\n",
    "            cols = []\n",
    "            for j in range(self.gridWidth):\n",
    "                if any(d.y == i and d.x == j for d in stenches):\n",
    "                    cols.append(1)\n",
    "                else:\n",
    "                    cols.append(0)   \n",
    "\n",
    "            rows.append(cols)\n",
    "\n",
    "        return rows \n",
    "    \n",
    "    #Build breeze location grid\n",
    "    def buildBreezeLocationGrid(self, breezes):\n",
    "        rows = []\n",
    "\n",
    "        for i in range(self.gridHeight):\n",
    "            cols = []\n",
    "            for j in range(self.gridWidth):\n",
    "                if any(d.y == i and d.x == j for d in breezes):\n",
    "                    cols.append(1)\n",
    "                else:\n",
    "                    cols.append(0)   \n",
    "\n",
    "            rows.append(cols)\n",
    "\n",
    "        return rows\n",
    "\n",
    "    #apply next action. Action value is passed in from training(0,1,2,4,5)\n",
    "    def nextAction(self, percept, action):\n",
    "        ret = deepcopy(self)\n",
    "\n",
    "        if(percept.stench == True):\n",
    "            ret.stenchLocations.append(ret.agentState.location)\n",
    "        if(percept.breeze == True):\n",
    "            ret.breezeLocations.append(ret.agentState.location) \n",
    "\n",
    "        ret.agentLocationGrid = self.buildAgentLocationGrid(ret.agentState.location)\n",
    "        ret.safeLocationGrid = self.buildSafeLocationGrid(ret.safeLocations)  \n",
    "        ret.stenchLocationGrid = self.buildStenchLocationGrid(ret.stenchLocations)       \n",
    "        ret.breezeLocationGrid = self.buildBreezeLocationGrid(ret.breezeLocations)\n",
    "        ret.agentHasGold =  1 if ret.agentState.hasGold == True else 0\n",
    "        ret.agentSensesGold = 1 if percept.glitter == True else 0\n",
    "        ret.agentHasArrow = 1 if ret.agentState.hasArrow == True else 0\n",
    "        ret.agentHeardScream = 1 if percept.scream == True else 0\n",
    "        ret.agentOrientationSet = self.buildAgentOrientatioSet(ret.agentState.orientation)\n",
    "\n",
    "        if action == 0:\n",
    "            ret.agentState = ret.agentState.forward(\n",
    "                self.gridWidth, self.gridHeight)\n",
    "            ret.safeLocations.append(ret.agentState.location)\n",
    "\n",
    "            return ret, Action.Forward\n",
    "        elif action == 1:\n",
    "            ret.agentState = ret.agentState.turnLeft()\n",
    "            return ret, Action.TurnLeft\n",
    "        elif action == 2:\n",
    "            ret.agentState = ret.agentState.turnRight()\n",
    "            return ret, Action.TurnRight\n",
    "        elif action == 3:\n",
    "            ret.agentState = ret.agentState.useArrow()\n",
    "            return ret, Action.Shoot\n",
    "        if action == 4:\n",
    "            if percept.glitter == True:\n",
    "                ret.agentState.hasGold = True\n",
    "\n",
    "            return ret, Action.Grab\n",
    "        if action == 5:\n",
    "            ret.agentState.isTerminated = True\n",
    "            return ret, Action.Climb                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "# cuda.init()\n",
    "# print(f\"is cuda available?: {torch.cuda.is_available()}\")\n",
    "# print(f\"What is the current device?: {torch.cuda.current_device()}\")\n",
    "# print(f\"Name of device: {cuda.Device(torch.cuda.current_device()).name()}\")\n",
    "# device = 'cpu'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karanteckwani/Projects/wumpus-world/wumpusWorld/model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'model')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "print(model_path)\n",
    "model_filename = 'deepQ_wumpus_model.pt'\n",
    "model_file_path = os.path.join(model_path, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mocZ3PEo00Op",
    "outputId": "d3681245-5220-43b0-ee47-4654190b7ab7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f8ff70717183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mnextMove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqval_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextMove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#method to return all belief state variables found in question 1 of the assignment description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f8ff70717183>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env, agent, percept, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cf3b870cb39d>\u001b[0m in \u001b[0;36mnextAction\u001b[0;34m(self, percept, action)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m#apply next action. Action value is passed in from training(0,1,2,4,5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnextAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstench\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0m_keep_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Make sure x lives at least as long as d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_wumpus/lib/python3.7/copy.py\u001b[0m in \u001b[0;36m_keep_alive\u001b[0;34m(x, memo)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \"\"\"\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# aha, this is the first one :-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt\n",
    "from collections import deque\n",
    "from random import randrange\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "l1 = 72 # number of features as defined by question 1. Returned from getAgentBeliefState()\n",
    "l2 = 150\n",
    "l3 = 100\n",
    "l4 = 6 # number of actions available -- all randomized within DeepQAgent Class\n",
    "start_time = time.time()\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "epochs = 1000\n",
    "losses = []\n",
    "max_moves = 500\n",
    "mem_size = 1000\n",
    "batch_size = 200\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "replay = deque(maxlen=mem_size)\n",
    "sync_freq = 500\n",
    "j=0\n",
    "won_counter = 0\n",
    "\n",
    "def get_model(path):\n",
    "    if not os.path.isfile(path):\n",
    "        print('new model setup from scratch')\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3,l4)\n",
    "        )\n",
    "    else:\n",
    "        print('Model loaded from filepath')\n",
    "        model = torch.load(path)\n",
    "    model2 = copy.deepcopy(model)\n",
    "    model2.load_state_dict(model.state_dict())\n",
    "\n",
    "    return model, model2\n",
    "\n",
    "def run(env, agent, percept, action):\n",
    "    agent, action = agent.nextAction(percept, action)\n",
    "    env, percept = env.applyAction(action)\n",
    "\n",
    "    return env, agent, percept\n",
    "\n",
    "def getState(belief_state):\n",
    "    state_ = np.array([belief_state]).reshape(1,l1) + np.random.rand(1,l1)/100.0   \n",
    "    state = torch.from_numpy(state_).float()\n",
    "\n",
    "    return state_, state\n",
    "\n",
    "model, model2 = get_model(model_file_path)\n",
    "# model.to(device)\n",
    "# model2.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epochs):\n",
    "    current = i\n",
    "    terminated = False\n",
    "    reward = 0\n",
    "    status = 1\n",
    "    mov = 0\n",
    "    world = WumpusWorldEnvironment()\n",
    "    initialEnv, initialPercept = world.apply(4, 4, 0.2, False)        \n",
    "    agent = DeepQAgent(4, 4)    \n",
    "    randGen = randrange(6)\n",
    "    env, agent, percept = run(initialEnv, agent, initialPercept, randGen)\n",
    "    #method to return all belief state variables found in question 1 of the assignment description\n",
    "    belief_state = agent.getAgentBeliefState()\n",
    "    state1_, state1 = getState(belief_state)\n",
    "\n",
    "    while terminated == False:\n",
    "        j+=1\n",
    "        mov += 1      \n",
    "#         state1 = state1.to(device)\n",
    "        qval = model(state1)\n",
    "#         qval = qval.to('cpu')\n",
    "        qval_ = qval.data.numpy()\n",
    "        action_ = np.argmax(qval_)\n",
    "\n",
    "        nextMove = action_.item()\n",
    "        if (random.random() < epsilon):\n",
    "            nextMove = np.random.randint(0,6)\n",
    "        else:\n",
    "            nextMove = np.argmax(qval_)\n",
    "\n",
    "        env, agent, percept = run(env, agent, percept, nextMove) \n",
    " \n",
    "        #method to return all belief state variables found in question 1 of the assignment description\n",
    "        belief_state = agent.getAgentBeliefState()\n",
    "        state2_, state2 = getState(belief_state)\n",
    "        reward += percept.reward\n",
    "\n",
    "        exp =  (state1, action_, reward, state2, percept.isTerminated)\n",
    "        replay.append(exp)\n",
    "        state1 = state2     \n",
    "\n",
    "        if len(replay) > batch_size:\n",
    "            minibatch = random.sample(replay, batch_size) \n",
    "            state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "            action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "            reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "            state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "            done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "            \n",
    "#             state1_batch = state1_batch.to(device)\n",
    "            Q1 = model(state1_batch) \n",
    "#             Q1 = Q1.to('cpu')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "#                 state2_batch = state2_batch.to(device)\n",
    "                Q2 = model2(state2_batch)\n",
    "#                 Q2 = Q2.to('cpu')\n",
    "                \n",
    "            \n",
    "            Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])\n",
    "            X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "            loss = loss_fn(X, Y.detach())\n",
    "            print(i, loss.item()) \n",
    "            print(env.visualize())\n",
    "            if i != epochs:\n",
    "                clear_output(wait=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "            \n",
    "            if j % sync_freq == 0: \n",
    "                model2.load_state_dict(model.state_dict())  \n",
    "            \n",
    "            if i % 200 == 0:\n",
    "                torch.save(model, model_file_path)\n",
    "            \n",
    "            if reward > 0:\n",
    "                won_counter += 1\n",
    "                print(f\"Win rate: {won_counter/epochs*100}%\")\n",
    "\n",
    "        if reward != -1:\n",
    "            mov = 0\n",
    "\n",
    "        if mov > max_moves:\n",
    "            terminated = True\n",
    "\n",
    "        if percept.isTerminated == True:\n",
    "            terminated = True\n",
    "\n",
    "losses = np.array(losses)            \n",
    "print(f\"training_time: {time.time() - start_time}\")\n",
    "print(f\"Win rate: {won_counter/epochs*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "f8X6wiTr_wM7",
    "outputId": "f7c2e27e-669c-44ce-fdbb-2850250a08eb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\",fontsize=22)\n",
    "plt.ylabel(\"Loss\",fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PIi_t3MG8a3"
   },
   "outputs": [],
   "source": [
    "def test_model(model, mode='static', display=True):\n",
    "    i = 0\n",
    "    world = WumpusWorldEnvironment()\n",
    "    initialEnv, initialPercept = world.apply(4, 4, 0.2, False)        \n",
    "    agent = DeepQAgent(4, 4)    \n",
    "    randGen = randrange(6)\n",
    "    env, agent, percept = run(initialEnv, agent, initialPercept, randGen)\n",
    "    #method to return all belief state variables found in question 1 of the assignment description\n",
    "    belief_state = agent.getAgentBeliefState()\n",
    "    state_, state = getState(belief_state)\n",
    "    \n",
    "    status = 1\n",
    "    reward = 0\n",
    "    while(status == 1): #A\n",
    "#         state = state.to(device)\n",
    "        qval = model(state)\n",
    "#         qval = qval.to('cpu')\n",
    "        qval_ = qval.data.numpy()\n",
    "        action_ = np.argmax(qval_)\n",
    "        action = action_.item()\n",
    "\n",
    "        env, agent, percept = run(env, agent, percept, nextMove) \n",
    " \n",
    "        #method to return all belief state variables found in question 1 of the assignment description\n",
    "        belief_state = agent.getAgentBeliefState()\n",
    "        state_, state = getState(belief_state)\n",
    "        reward += percept.reward        \n",
    "        #print(percept.show())\n",
    "        if reward != -1:\n",
    "            if reward > 0:\n",
    "                status = 2                \n",
    "                if display:\n",
    "                    print(\"Game won! Reward: %s\" % (reward,))\n",
    "            else:\n",
    "                status = 0\n",
    "                if display:\n",
    "                    print(\"Game LOST. Reward: %s\" % (reward,))\n",
    "        i += 1\n",
    "        if (i > 15):\n",
    "            if display:\n",
    "                print(\"Game lost; too many moves.\")\n",
    "            break\n",
    "    \n",
    "    print(env.visualize())\n",
    "    clear_output(wait=True)\n",
    "    print(\"-----------------------\")\n",
    "    if status == 2:\n",
    "        win = True\n",
    "    else:\n",
    "        win = False\n",
    "    return win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNEaN1L5In4B",
    "outputId": "ab8c6ff8-ffef-42b3-d2ac-851f33c7e9b6"
   },
   "outputs": [],
   "source": [
    "max_games = 1000\n",
    "wins = 0\n",
    "\n",
    "for i in range(max_games):\n",
    "    win = test_model(model, mode='random', display=False)\n",
    "    if win:\n",
    "        wins += 1\n",
    "win_perc = float(wins) / float(max_games)\n",
    "print(\"Games played: {0}, # of wins: {1}\".format(max_games,wins))\n",
    "print(\"Win percentage: {}%\".format(100.0*win_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wumpus-world.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
